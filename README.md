# Attention Implementation

I tried to implement the Transformer network architecture using pytorch, this architecture is based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.

<p align="center"><img src="Images/Transformer.PNG" > </p>

### Connect with me:

<p>
  <a href="https://www.linkedin.com/in/taha-tamir-351272145/" rel="nofollow noreferrer">
    <img src="https://i.stack.imgur.com/gVE0j.png" alt="linkedin"> LinkedIn
  </a> &nbsp;
 </p